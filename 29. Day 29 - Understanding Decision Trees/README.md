
## **DAY 29 (17 Oct 2023):**
### Topic: Understanding Decision Trees

**Key Takeaways:**
1. **Entropy:** It measures the impurity or randomness of a dataset. Lower entropy indicates that the dataset is more uniform and easier to predict.

2. **Information Gain:** It quantifies the effectiveness of a particular attribute in classifying the dataset. It's the difference between the entropy before and after the dataset is split on an attribute.

3. **Gini Impurity:** Similar to entropy, Gini impurity measures the degree of probability of a particular element being classified incorrectly when chosen randomly. So lower the gini impurity, the dataset is more uniform/clean.

4. **Greedy Approach:** It does the best split at a given step at that point of time rather than looking for splitting a step for a better tree in upcoming steps!

5. **Pruning:** It is a technique used to reduce the size of decision trees by removing sections of the tree that provide little power to classify instances.

Understanding these concepts deeply is crucial for developing a solid grasp of decision trees and their applications in machine learning. Check out the notes for more.

LinkedIn post: [Day 29 Update](https://www.linkedin.com/feed/update/urn:li:activity:7120331323759579136?utm_source=share&utm_medium=member_desktop)

---