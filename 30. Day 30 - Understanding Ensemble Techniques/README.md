
## **DAY 30 (18 Oct 2023):**
### Topic: Understanding Ensemble Techniques

ðŸš€**Key Takeaways:**
1. **Bagging:** Bagging and Pasting employ the collective wisdom of multiple models trained on different subsets of the data. These methods aim to reduce variance and improve model accuracy by leveraging the concept of averaging predictions from various models.

2. **Pasting:** Pasting employs random sampling without replacement, while Bagging uses random sampling with replacement.

3. **Boosting:** Boosting sequentially improves model performance by assigning more weight to misclassified data points. This technique essentially trains models in a serial manner, where each subsequent model corrects the errors made by the previous ones, resulting in an ensemble model with stronger predictive capabilities.

4. **Stacking:** Stacking involves combining multiple models and allowing a meta-model to learn how to best leverage individual model predictions. This approach aims to boost model performance by leveraging the diverse expertise of various models. Stacking essentially combines the strengths of multiple models to create a more accurate and robust predictive model.

LinkedIn post: [Day 30 Update](https://www.linkedin.com/posts/ravi6123_understanding-ensemble-techniques-handwritten-activity-7120472401028530176-Q2DF?utm_source=share&utm_medium=member_desktop)

---