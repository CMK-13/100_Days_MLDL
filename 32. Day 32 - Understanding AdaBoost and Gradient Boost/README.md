
## **DAY 32 (20 Oct 2023):**
### Topic: Understanding Boosting Algorithms

1. Concept of Boosting
2. Understanding Ada Boost
3. Solving an Example on AdaBoost
4. Understanding Gradient Boosting
5. Solving an Example on Gradient Boosting
6. AdaBoost vs Gradient Boosting

Key Takeaways:
1. AdaBoost: AdaBoost adjusts the weights of misclassified samples to improve model performance, emphasizing error correction for better accuracy.

2. Gradient Boosting: This method builds strong predictive models by iteratively minimizing errors using gradients of the loss function.

3. Comparing AdaBoost and Gradient Boosting: While AdaBoost focuses on adjusting sample weights, Gradient Boosting minimizes errors using gradient-based optimization, making it more suitable for complex datasets.
Checkout the notes for more.

Detailed Notes: [Day 32 Commit](https://github.com/ds-teja/100_Days_MLDL/tree/main/32.%20Day%2032%20-%20Understanding%20AdaBoost%20and%20Gradient%20Boost)

LinkedIn post: [Day 32 Update](https://www.linkedin.com/feed/update/urn:li:activity:7121335130786316288?utm_source=share&utm_medium=member_desktop)

---