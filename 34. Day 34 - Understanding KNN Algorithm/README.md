
## **DAY 34 (22 Oct 2023):**
### Topic: Understanding K Nearest Neighbours

1. How does K-Nearest Neighbours work
2. How is Distance Calculated
   - Eculidean Distance
   - Hamming Distance
   - Manhattan Distance
3. Why is KNN a Lazy Learner
4. Effects of Choosing the value of K
5. Different ways to perform KNN
6. Understanding KD-Tree
7. Solving an Example of KD Tree
8. Understanding Ball Tree

Key Takeaways:

1. KNN is a non-parametric, instance-based learning algorithm used for both classification and regression tasks, considering similarity measures to predict outcomes.

2. It's a simple, easy-to-understand algorithm with no training phase, which is why also called Lazy Learner, making it effective for quick predictions and easy implementation in various contexts.

3. KNN's principle lies in finding the K closest training examples in the feature space to the new point, and the majority class among the K nearest neighbors determines the class of the new point.

4. KNN works well with a small number of input variables but might not be suitable for datasets with many features or high dimensions due to the curse of dimensionality. Its computational cost is high for large datasets and is sensitive to noisy data.

5. The k-d tree partitions the space into regions and stores the data points in a hierarchical structure, facilitating fast nearest-neighbor searches and range searches. Check out the notes for more.

By grasping the underlying mechanisms and limitations of the K-Nearest Neighbors Algorithm, we can better utilize its strengths and understand its applicability in different data science contexts.


Detailed Notes: [Day 34 Commit](https://github.com/ds-teja/100_Days_MLDL/tree/main/34.%20Understanding%20KNN%20Algorithm)

LinkedIn post: [Day 34 Update](https://www.linkedin.com/feed/update/urn:li:activity:7121940916298244096?utm_source=share&utm_medium=member_desktop)

---